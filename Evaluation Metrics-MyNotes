Evaluation metrics for Classification ->

Confusion Matrix 
Accuracy
Alternatives to Accuracy
Precision and Recall
F-Score
AUC-ROC
Log Loss
Gini Coefficient

Evaluation metrics for Regression ->

MAE(Mean Absolute Error)
MSE(Mean Square Error)
RMSE(Root Mean Square Error)
RMSLE(Root Mean Square Log Error)
R2 and Adjusted R2


1. Confusion Matrix -> A Confusion Matrix is a performance evaluation tool for classification problems. It shows the number of correct and incorrect predictions 
made by the model compared to the actual outcomes (ground truth).
Note : In a confusion matrix, n typically represents:
The total number of samples or observations used to generate the matrix.

Eg Binary - >      Predicted Values
                    P      N
Actual values   P   TP    FN
                N   FP    TN
Total Actual Positive -> TP + FN
Total Actual Negative -> FP + TN 

Example ->
| Sample | **Actual** | **Predicted** | Notation
| ------ | ---------- | ------------- |
| 1      | 1          | 1             |TP      -> First check the Predicted -> 0 -False, 1 ->True, then if the predicted and actual is true T, else F
| 2      | 0          | 0             |TN
| 3      | 1          | 0             |FN
| 4      | 1          | 1             |TP
| 5      | 0          | 1             |FP
| 6      | 0          | 0             |TN
| 7      | 1          | 1             |TP
| 8      | 0          | 0             |TN
| 9      | 1          | 0             |FN
| 10     | 0          | 1             |FP

2. Accuracy -> Total number of correct(correct predictions) / total number of observations(total predictions)
 -> ( TP + TN)/(TP + TN +FP +FN)

In accuracy we should consider bias , because even though there are a lot of false cases, the accuracy as per the formula remains high, but the whole goal of predicting the outcomes changes.

True Postive Rate ( TPR) -> TP/TP + FN  -> higher the value - better the model. 
False Negative Rate (FNR) -> FN /TP +FN -> lower the value - better the model
True Negative Rate (TNR)  -> TN/FP + TN -> higher the value - better the model  - In other words, how well your model identifies the "No" class.
False Negative Rate (FNR) -> FN/FP + TN -> lower the value - better the model  - It measures the proportion of actual negatives that were incorrectly predicted as positive.

All these values range from 0 to 1.


3. Precision and Recall -> Out of all the postive prediction how many are actually postive. -> Predictions Actually Positive/ Total Predicted Postive
Precision = TP/TP + FP  ( we proritze false postive ..) 

Recall -> Out of all the actual postive, how many are predicted positive
Recall = TP/TP + FN (( we proritze false negative ..))

High Precision - Low Recall and vice versa ( its a oppositve curve type graphically )


4. F1 - Score -> F-measure" or "F-score -> F1 Score = Harmonic mean of Precision and Recall (max when precision = recall ) 
F1-Score = 2/(1/Precision + 1/Recall)

5. Threshold -> For models which dont have whole values ( or 0 and 1's ) we can use the the threshold value as a limit to set the values to 0 and 1.
Example -> if we have v>=0.5 ->1 ; otherwise 0  -> we can further play around with it while checking the true postive rate.

6. AUC-ROC ->  AUC -> Area under the curve ; ROC -> Receiver Operating Characteristic ->
Evaluation for binary classification.
Gives the trade off between True Postives and False Positives
Note : ( more the area under the curve better the model)  -> (TPR should be greater than FPR) 

For a model steps to measure the points to TPR and FPR ->
1. Arrrange the probabilities in Decreasing order
2. Take the first probability as threshold
3. Calculate the TPR and FPR 
4. Repeat ; for all the next values

7. Log Loss -> the negative average of log of the corrected probabilities of each instance.
Firstly we consider the corrected probabilities which means, the calcuate the probabilities for class =1.
then we calculate the log base e of these values, then multiply by negative.

 8. The formula for Gini coefficient: Gini = 2*AUC – 1 
The formula for Gini coefficient: Gini = 2*AUC – 1
Note : Gini coefficient is different from the Gini Index (1- gini) we ecnounter in Decision Trees.



Regression Metrics ->
Error -> how far are observed values are from the actual( i.e the line in the graph)
1. MAE(Mean Absolute Error)         -> MAE=n1​i=1∑n​∣yi​−y^​i​∣
2. MSE(Mean Square Error)           -> MSE=n1​i=1∑n​(yi​−y^​i​)2
3. RMSE(Root Mean Square Error)     -> RMSE=n1​i=1∑n​(yi​−y^​i​)2
4. RMSLE(Root Mean Square Log Error)-> RMSLE=n1​i=1∑n​(log(1+y^​i​)−log(1+yi​))2
5. R2                               -> R2=1−∑(yi​−yˉ​)2∑(yi​−y^​i​)2​  -> higher then better the model performance
| Symbol      | Meaning                             |
| ----------- | ----------------------------------- |
| y_i      | Actual value for the i-th sample    |
| hat{y}_i | Predicted value for the i-th sample |
| bar{y}   | Mean of actual values               |
| n        | Number of observations (samples)    |

6. Adjusted R2                      -> Adjusted R2=1−(1−R2)⋅n−k−1n−1​  -> lower then better the model performance
-> Adjusts R² for the number of predictors (k) and samples (n)
-> Penalizes adding irrelevant features


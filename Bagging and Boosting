As we have to starta online marketing for similar products irrespective of their segments.
The problem statement ->
1. Identify the factors to predict units sold for products. - supervised
2. Identify which product will sellmore than 1000 products. - supervised
3. Create a new age bundled online marketing stratergy.     - unsupervised

Ensemble/Ensembling ->
Combining mutiple machine learning algorithms to get better accuracy, efficiency and robustness.
Types ->
1.Homogenuous ensemble models   -> Only one model is used : Techniques like bagging and boosting.
2.Heterogenuous ensemble models -> More than 1 model      : Techniques like voting, stacking, blending

Homogenuous ensemble models -> Same models -> Bagging 
Bagging -> Bootstrap + Aggregation
Bootstrap -> Create mutiple samples of the same dataset with replacament -> 
used to creates full diversity of the main dataset -> Randomness(each subset is Unique due to randomness) + variability( ensures each model captures diverse information)

Aggregation -> 
1. Classification : Mode of Predictions
2. Regression     : Mean of Predictions

How bagging battles overfitting -> many decision trees algo for each partial dataset then the average of that helps in getting a good estimate ruling out overfitting.
Best for situation like 1. High variance , 2. Low Bias

Bagging - Random Forest implementation ->
train_df.values -> .values gives a NumPy array, but if your DataFrame has mixed data types, pandas will upcast to a common type
    .to_numpy() ->  better use case is using .to_numpy()
Both give the same output ->
[3260.0, 24.70360480640855, 'Makeup', ..., 0.0, 0.0, 1],
       [3729.0, 22.22651448639157, 'Makeup', ..., 0.0, 0.0, 1],
       [2675.0, 25.57446808510638, 'Makeup', ..., 0.0, 0.0, 1],
      [375.0, 43.64347826086956, 'Skincare', ..., 7108033.0, 2.0, 0]],
      dtype=object)

The we calculate the Normal Case for the rating, (Rating - 1,2,3,4,5) - > Rating(4,5) / Rating (1,2)
if Rating(1,2,3,4) -> 0 then we replace the value with 0.0
if Rating(1,2)     -> 0 then we replace the value with -99999
(Storing all the normal values in rating ratio) : 
The case where rating is -9999-> we replace that with the max rating of that max of the rating_ratio loop.
Then we create a coloumn -> 'Good_By_Bad_Rating' -> add the Rating_Ratio to it.
Then drop all the rating coloumns(1,2,3,4,5) 

Same for training and validation.

One-hot encoding ->
train_df = pd.get_dummies(train_df, columns = ['Segment'], prefix = 'Segment', drop_first = True)
pd.get_dummies() -> Converts categorical variables into one-hot encoded columns — creating new columns with 0s and 1s indicating the presence of each category.

columns=['Segment'] -> Specifies that the Segment column in train_df should be transformed into dummy variables.

prefix='Segment' -> Adds the prefix "Segment" to the new dummy column names, so the new columns will look like "Segment_<category>".
                    This makes it clear which original column the new dummy columns come from.

drop_first=True -> Drops the first dummy column created to avoid multicollinearity (called the dummy variable trap).
                   This means if your Segment column has 3 categories, only 2 dummy columns will be created, and the dropped category is the baseline.

Result -> The original Segment column is removed and replaced by these new dummy columns, which can now be used as numerical features for machine learning models.

Product  	Segment	  Units_sold
P1	      Makeup	    100
P2	      Skincare	  200
P3	      Haircare	  150
P4	      Makeup	    120

Product	Units_sold	Segment_Skincare	Segment_Haircare
P1	      100            	0	                0
P2	      200	            1	                0
P3	      150	            0	                1
P4	      120	            0	                0

in f1_score -> {round(f1*100, 2)}%" -> mupliplying by 100 for percentage, then rounding of to 2 decimal places

as we got f1_score on training dataset as 100% , the model is overfitting to help to avoid this we,
model = DecisionTreeClassifier(max_depth=20, min_samples_leaf=10, random_state=42)
and then we check if the code is not overfittiing and it works !!!

from sklearn.ensemble import BaggingClassifier 

then we use ->
bagging_classifier = BaggingClassifier(base_classifier,n_estimators=101,random_state=42, n_jobs=-1)
Explanation ->
BaggingClassifier is an ensemble method that creates many copies of a base model (here called base_classifier), trains each on random subsets of the data, and combines their predictions to improve accuracy and reduce overfitting.
base_classifier: The model you want to bag (e.g., a decision tree).
n_estimators=101: Number of base models (estimators) to train and aggregate — here, 101 trees/models.(use odd number to avoid ties in mode value)
random_state=42: Fixes the randomness for reproducibility.
n_jobs=-1: Uses all available CPU cores for parallel training to speed up computation.

Random Forest -> mutiple decision trees ->
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(
    n_estimators=100,
    max_depth=None,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='auto',
    bootstrap=True,
    random_state=42,
    n_jobs=-1
)

Explanation of each attribute ->
n_estimators      : Number of trees in the forest. More trees usually improve performance but increase computation time.
max_depth         : Maximum depth of each tree. Limits how deep each tree can grow to prevent overfitting.
min_samples_split : Minimum number of samples required to split an internal node. Higher values make the tree more conservative.
min_samples_leaf  : Minimum number of samples required to be at a leaf node. Helps smooth the model.
max_features      : Number of features to consider when looking for the best split. Controls randomness and diversity among trees.
bootstrap         : Whether to use bootstrap samples (sampling with replacement) when building trees. Typically True.(automatically set to true)
random_state      : Fixes randomness to ensure reproducible results.
n_jobs            : Number of CPU cores to use (-1 means use all cores for parallel training).

Difference between BaggingClassifier and RandomForestClassifier -> Random forest selects only few features(subsets) and works on them.
where as in bagging we have to mention it to select random features in the attributes.

2 key features of RandomForest which are not part of the decision tree ->
1. n_estimators      -> Estimates how many individual decision trees will be build in the forest.
2. out_of_bag scores -> Build in, to estimate the exact(optimum) number of trees.

In general, out of 100% , 33% of the samples(i.e features) are not selected by the decision trees.(these are the out of bag samples)

Note : To access a attribute in a function we _ at the end.
Example -> clf = RandomForestClassifier(oob_score=True, random_state=42, n_jobs=-1) 
Now to access oob_score we use -> clf.oob_score_, or clf.n_jobs_ .....

Note : clf.set_params(n_estimators=n_estimators, max_depth=20, min_samples_leaf=10) ->
set_params() is used to set the hyper parameters of the model after being created!

We use a loop to iterate of over the n_samples created and plot the grapgh for both 
oob_scores and train_score(train_score = clf.score(X_train, y_train)) -> to get a graph to better understand how the train works with oob_score.

Note : in strings ->
1. (<15) -> print(f"{n_estimators:<15})
< → Left-align the value.
15 → Use a total width of 15 characters.

2.(<15.4f) -> print(f"{train_score:<15.4f})
< → Left-align the number.
15.4f → It's a float with:
15 total characters wide,
4 digits after the decimal point.

3. (.4f) -> print(f"{oob_score_val:.4f})
No alignment or width, just format as:
A float with 4 digits after the decimal point.

Bagging regression model - >
# Create a Bagging Regressor with Decision Tree as base
bagging_regressor = BaggingRegressor(
    base_estimator=DecisionTreeRegressor(max_depth=9, min_samples_leaf=30),
    n_estimators=50,
    random_state=42
)

# Fit the Bagging regressor on the training data
bagging_regressor.fit(X_train, y_train)

r2_score -> used or regression models:
Metric	                                      Task Type	                                                Purpose	Output Range
r2_score	    Regression	Measures how well predicted values fit the actual data (goodness of fit)	 -∞ to 1 (1 = perfect)
f1_score	    Classification	Balances precision and recall, especially for imbalanced classes          0 to 1 (1 = perfect)

Example ->
from sklearn.metrics import r2_score

# Actual values (true house prices in thousands)
y_true = [100, 200, 300, 400, 500]
# Predicted values from a regression model
y_pred = [110, 190, 290, 410, 490]
# Calculate R² score
r2 = r2_score(y_true, y_pred)
print(f"R2 Score: {r2:.2f}")

Output ->
R2 Score: -1.50

math behind it->
y_true = [100, 200, 300, 400, 500]
y_pred = [110, 190, 290, 410, 490]
Average true value: 300

(100 - 110)² = 100
(200 - 190)² = 100
(300 - 290)² = 100
(400 - 410)² = 100
(500 - 490)² = 100
Total errors = 500

Distance from average (difference squared):

(100 - 300)² = 40000
(200 - 300)² = 10000
(300 - 300)² = 0
(400 - 300)² = 10000
(500 - 300)² = 40000
Total distance = 100000

R² = 1 − (500 ÷ 100000) = 1 − 0.005 = 0.995

Prediction explanation ->
R² close to 1 means predictions are very close to true values.
R² close to 0 means model is not better than just guessing the average.
(If R² is close to 0, it means your model’s predictions are about as good as just guessing the average of all the actual values.)


Boosting ->

